{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part1 - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import boto\n",
    "from boto.s3.key import Key\n",
    "import zipfile\n",
    "import boto.s3.connection\n",
    "from os.path import basename\n",
    "color = sns.color_palette()\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from uszipcode import ZipcodeSearchEngine\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the properties and transaction details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detail_2016 = pd.read_csv('/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/properties_2016.csv',low_memory=False)\n",
    "trans_2016 = pd.read_csv('/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/train_2016.csv',low_memory=False)\n",
    "details_2017 = pd.read_csv('/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/properties_2017.csv',low_memory=False)\n",
    "trans_2017 = pd.read_csv('/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/train_2017.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.590064679007\n",
      "2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sonalichaudhari/anaconda/lib/python3.5/site-packages/pandas/core/generic.py:3295: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/sonalichaudhari/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.574331578471\n",
      "2017\n",
      "Combined CSV created successfully!!!\n"
     ]
    }
   ],
   "source": [
    "df1=Cleaning(detail_2016,trans_2016)\n",
    "df2= Cleaning(details_2017,trans_2017)\n",
    "Merge(df1,df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89611, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Cleaning(detail,trans):\n",
    "    # 70% missing data\n",
    "    missing_data = detail.isnull().sum().to_frame(name='Missing_Count').reset_index()\n",
    "    missing_data['Missing_percent'] = (missing_data['Missing_Count']/detail.shape[0])*100\n",
    "    missing_data.head()\n",
    "    missing_data = missing_data.sort_values(by='Missing_percent', axis=0, ascending=True)\n",
    "    features = missing_data['index'][(missing_data.Missing_percent <30)]\n",
    "    detail = detail[features]\n",
    "\n",
    "    # Separating the year and month in transaction dataframe\n",
    "    trans['transactiondate'] = pd.to_datetime(trans['transactiondate'])\n",
    "    trans['year'] = trans['transactiondate'].dt.year\n",
    "    trans['month'] = trans['transactiondate'].dt.month\n",
    "\n",
    "    # Merging the properties and transaction data into a single dataframe\n",
    "    df =   pd.merge(trans,detail,on='parcelid', how ='left')\n",
    "\n",
    "    # Getting rid of redudndant columns\n",
    "    del df['censustractandblock']\n",
    "    del df['fullbathcnt']\n",
    "    del df['calculatedbathnbr']\n",
    "    del df['finishedsquarefeet12']\n",
    "\n",
    "    # Latitude and longitude format \n",
    "    df = df.dropna(subset=['latitude'])\n",
    "    df = df.dropna(subset=['longitude'])\n",
    "    df['latitude'] = df['latitude'] / 1000000\n",
    "    df['longitude'] = df['longitude'] / 1000000\n",
    "\n",
    "    # Zipcode\n",
    "    z = df[['regionidzip','regionidcounty']][df.regionidcounty ==3101.0]\n",
    "    search = ZipcodeSearchEngine()\n",
    "    zipcode = z['regionidzip'][z.regionidzip <= 100000].mode().values[0]\n",
    "    df.loc[df['regionidzip'] > 100000, 'regionidzip'] = zipcode\n",
    "\n",
    "    df.loc[df.regionidzip.isnull(), 'regionidzip']=search.by_coordinate(df['latitude'].values[0],df['longitude'].values[0], radius=30, returns=1)[0]['Zipcode']\n",
    "\n",
    "    # propertycountylandusecode\n",
    "    df['propertycountylandusecode'] = df['propertycountylandusecode'].apply(lambda x: np.nan if x == 0 else x) \n",
    "    df['propertycountylandusecode'] = df['propertycountylandusecode'].fillna(df['propertycountylandusecode'].mode()[0]) \n",
    "    lb_make = LabelEncoder()\n",
    "    df[\"propertycountylandusecode\"] = lb_make.fit_transform(df[\"propertycountylandusecode\"])\n",
    "    df[[\"propertycountylandusecode\"]]\n",
    "    lb_makeDesc = LabelEncoder()\n",
    "    lb_makeDesc = LabelEncoder()\n",
    "    df[\"transactiondate\"] = lb_makeDesc.fit_transform(df[\"transactiondate\"])\n",
    "\n",
    "    df['taxvaluedollarcnt'].fillna(df['taxvaluedollarcnt'].median(), inplace=True)\n",
    "    df['landtaxvaluedollarcnt'].fillna(df['landtaxvaluedollarcnt'].median(), inplace=True)\n",
    "    df['taxamount'].fillna(df['taxamount'].median(), inplace=True)\n",
    "\n",
    "    # yearbuilt\n",
    "    knn_year(df)\n",
    "\n",
    "    # age\n",
    "    df['age'] =2017-df['yearbuilt']\n",
    "\n",
    "    # calculatedfinishedsquarefeet\n",
    "    df = df[df.calculatedfinishedsquarefeet>=120]\n",
    "    df['structuretaxvaluedollarcnt'].fillna(df['structuretaxvaluedollarcnt'].median(), inplace=True)\n",
    "\n",
    "    # lotsizesquarefeet\n",
    "    print(df.year[0])\n",
    "    if df.year[0] ==2017:\n",
    "        out = 2000000\n",
    "    else:\n",
    "        out = 3000000\n",
    "    lot_median =df['lotsizesquarefeet'].median()\n",
    "    df.loc[df.lotsizesquarefeet > out, 'lotsizesquarefeet']= lot_median\n",
    "    df['lotsizesquarefeet'].fillna(df['lotsizesquarefeet'].median(), inplace=True)\n",
    "\n",
    "    # regioncityid\n",
    "    missingcity(df)\n",
    "    path = '/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/New_Data/'\n",
    "    path = path+'CSV'+str(df.year[0])+'.csv'\n",
    "    df.to_csv(path, sep=',')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# path = '/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/New_Data/'\n",
    "# path = path+'CSV'+str(df.year[0])+'.csv'\n",
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def knn_year(yeardf):\n",
    "    whole = ['latitude','longitude','yearbuilt','parcelid']\n",
    "    base = ['latitude','longitude']\n",
    "    target_column = 'yearbuilt'\n",
    "\n",
    "    null = yeardf[target_column].isnull()\n",
    "    not_null = ~null\n",
    "    num_miss = null.sum()\n",
    "    if num_miss ==0:\n",
    "        print('No Null Values!!!')\n",
    "    else:\n",
    "        X_train = yeardf.loc[not_null, whole].sample(frac = 1)\n",
    "\n",
    "        # X_train.latitude.astype(int)\n",
    "        # X_train.longitude.astype(int)\n",
    "        # X_train.yearbuilt.astype(int)\n",
    "        X=X_train[base].values\n",
    "\n",
    "        y = X_train[target_column].astype(int)\n",
    "        y = y.values\n",
    "\n",
    "        Y_target =yeardf.loc[null, whole].sample(frac = 1)\n",
    "        Y_target.latitude.astype(int)\n",
    "        Y_target.longitude.astype(int)\n",
    "\n",
    "        Y =Y_target[['latitude','longitude']].values\n",
    "\n",
    "        clf = neighbors.KNeighborsClassifier()\n",
    "        clf.fit(X,y)\n",
    "        accuracy = clf.score(X, y)\n",
    "        print(accuracy) \n",
    "\n",
    "        #Y = Y.reshape(len(X), -1)\n",
    "        prediction = clf.predict(Y)\n",
    "        Y_target.loc[Y_target.yearbuilt.isnull(),'yearbuilt'] = prediction\n",
    "        yeardf.loc[yeardf.parcelid.isin(Y_target.parcelid), ['yearbuilt']] = Y_target[['yearbuilt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def missingcity(m):\n",
    "    city = m[['regionidcounty','parcelid', 'regionidcity']]\n",
    "    Mode = m['regionidcity'].mode()[0]\n",
    "    county= list(city['regionidcounty'].unique())\n",
    "\n",
    "    for c in county:\n",
    "        temp = city[city.regionidcounty == c]\n",
    "        if(temp.shape == temp[temp.isnull().any(axis=1)].shape ):\n",
    "                temp['regionidcity'].fillna(Mode, inplace=True)\n",
    "        else:\n",
    "                c = temp['regionidcity'].mode()[0]\n",
    "                temp['regionidcity'].fillna(c, inplace=True)\n",
    "        city.loc[city.parcelid.isin(temp.parcelid), ['regionidcity']] = temp[['regionidcity']]\n",
    "    city['regionidcity'].fillna(city['regionidcity'].mode()[0], inplace = True)      \n",
    "    m.loc[m.parcelid.isin(city.parcelid), ['regionidcity']] = city[['regionidcity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading csv to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID=''\n",
    "AWS_SECRET_ACCESS_KEY=''\n",
    "try:\n",
    "    connect = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "except:\n",
    "    print(\"incorrect aws access key and/or secret key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connection = boto.connect_s3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "#csvFile = 'zillowData.csv'\n",
    "zipname = \"zillowdata.zip\"\n",
    "z = zipfile.ZipFile(zipname, \"w\")\n",
    "z.write('zillowData.csv')\n",
    "cur_path\n",
    "bucket_name = 'zillowdatampsc'\n",
    "\n",
    "loc = boto.s3.connection.Location.USWest\n",
    "\n",
    "bucket = connect.create_bucket(bucket_name, location=loc)\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "s3key = boto.s3.key.Key(bucket)\n",
    "s3key.key = zipname\n",
    "s3key.set_contents_from_filename(zipname, cb=percent_cb, num_cb=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Merge(data1, data2):\n",
    "    df_concat = pd.concat([data1, data2]).reset_index()\n",
    "    del df_concat['index']\n",
    "    df_concat.to_csv('/Users/sonalichaudhari/Desktop/ADS_MidTerm/Data/New_Data/Clean_Combined.csv', sep=',')\n",
    "    print('Combined CSV created successfully!!!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
